name: Continuous Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:  # Allow manual trigger

env:
  PYTHON_VERSION: "3.10"
  CACHE_NUMBER: 0  # Increment to reset cache

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_NUMBER }}-${{ hashFiles('requirements.txt', 'pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_NUMBER }}-
            ${{ runner.os }}-pip-

      - name: Cache pre-commit hooks
        uses: actions/cache@v4
        with:
          path: ~/.cache/pre-commit
          key: ${{ runner.os }}-pre-commit-${{ env.CACHE_NUMBER }}-${{ hashFiles('.pre-commit-config.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pre-commit-${{ env.CACHE_NUMBER }}-
            ${{ runner.os }}-pre-commit-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install pre-commit
          # Install dev dependencies
          pip install black ruff mypy pytest pytest-cov
          # Install type stubs
          pip install types-requests types-PyYAML types-setuptools

      - name: Run pre-commit hooks
        run: |
          pre-commit run --all-files --show-diff-on-failure

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: code-quality

    strategy:
      matrix:
        python-version: ["3.10", "3.11"]
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-test-${{ matrix.python-version }}-${{ env.CACHE_NUMBER }}-${{ hashFiles('requirements.txt', 'pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-test-${{ matrix.python-version }}-${{ env.CACHE_NUMBER }}-
            ${{ runner.os }}-pip-test-${{ matrix.python-version }}-

      - name: Cache test data
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/huggingface
            ./test_data_cache
          key: ${{ runner.os }}-test-data-${{ env.CACHE_NUMBER }}
          restore-keys: |
            ${{ runner.os }}-test-data-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          # Install test dependencies
          pip install pytest pytest-cov pytest-xdist pytest-timeout
          # Install project dependencies (when requirements.txt is available)
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # Install package in editable mode
          pip install -e .

      - name: Run unit tests with coverage
        run: |
          # Run tests with coverage
          pytest tests/unit \
            --cov=core \
            --cov-report=term-missing \
            --cov-report=html:htmlcov \
            --cov-report=xml:coverage.xml \
            --cov-fail-under=70 \
            -v \
            --tb=short \
            --maxfail=5 \
            --timeout=300

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports-${{ matrix.python-version }}
          path: |
            htmlcov/
            coverage.xml

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-${{ matrix.python-version }}
          fail_ci_if_error: false

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: unit-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-integration-${{ env.CACHE_NUMBER }}-${{ hashFiles('requirements.txt', 'pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-integration-${{ env.CACHE_NUMBER }}-
            ${{ runner.os }}-pip-integration-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install pytest pytest-cov pytest-xdist pytest-timeout
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install -e .

      - name: Run integration tests
        run: |
          # Run integration tests (if they exist)
          if [ -d "tests/integration" ]; then
            pytest tests/integration \
              -v \
              --tb=short \
              --maxfail=3 \
              --timeout=600
          else
            echo "No integration tests found, skipping..."
          fi

  reproducibility-check:
    name: Reproducibility System Check
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: code-quality

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-repro-${{ env.CACHE_NUMBER }}-${{ hashFiles('requirements.txt', 'pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-repro-${{ env.CACHE_NUMBER }}-
            ${{ runner.os }}-pip-repro-

      - name: Install minimal dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          # Install only required packages for reproducibility demo
          pip install pyyaml gitpython psutil
          
      - name: Run reproducibility demo
        run: |
          # Run the reproducibility demo in offline mode
          export PIXELIS_OFFLINE_MODE=true
          bash scripts/demo_reproducibility.sh --offline --capture-level 1

      - name: Check artifacts created
        run: |
          # Verify that artifacts were created
          if [ -d "runs" ] && [ -d "artifact_cache" ]; then
            echo "✓ Reproducibility system created artifacts successfully"
            ls -la runs/ | head -10
            ls -la artifact_cache/ | head -10
          else
            echo "✗ Reproducibility system failed to create artifacts"
            exit 1
          fi

  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: code-quality

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety pip-audit

      - name: Run Bandit security scan
        run: |
          bandit -r core scripts -ll -f json -o bandit-report.json || true
          # Pretty print the report
          python -m json.tool bandit-report.json || true

      - name: Check for known vulnerabilities
        run: |
          # Check dependencies for known vulnerabilities
          if [ -f requirements.txt ]; then
            pip-audit -r requirements.txt --desc || true
          fi

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json

  build-docker:
    name: Build Docker Image
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [unit-tests, integration-tests]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          # Build Docker image if Dockerfile exists
          if [ -f Dockerfile ]; then
            docker build -t pixelis:ci-${{ github.sha }} .
            echo "✓ Docker image built successfully"
          else
            echo "No Dockerfile found, skipping Docker build"
          fi

  all-checks-passed:
    name: All CI Checks Passed
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, reproducibility-check, security-scan]
    if: always()

    steps:
      - name: Verify all checks passed
        run: |
          if [ "${{ needs.code-quality.result }}" != "success" ] || \
             [ "${{ needs.unit-tests.result }}" != "success" ] || \
             [ "${{ needs.integration-tests.result }}" != "success" ] || \
             [ "${{ needs.reproducibility-check.result }}" != "success" ] || \
             [ "${{ needs.security-scan.result }}" != "success" ]; then
            echo "❌ One or more CI checks failed"
            echo "Code Quality: ${{ needs.code-quality.result }}"
            echo "Unit Tests: ${{ needs.unit-tests.result }}"
            echo "Integration Tests: ${{ needs.integration-tests.result }}"
            echo "Reproducibility: ${{ needs.reproducibility-check.result }}"
            echo "Security Scan: ${{ needs.security-scan.result }}"
            exit 1
          else
            echo "✅ All CI checks passed successfully!"
          fi

# Future optimization notes:
# 1. As the test suite grows, consider parallelizing test execution using matrix strategy
# 2. Add GPU tests on self-hosted runners when available
# 3. Implement incremental testing to only run affected tests
# 4. Add performance regression testing
# 5. Consider adding visual regression testing for UI components