**Round 3: Human Evaluation of Reasoning Quality**

*   **Task 1: Define the Evaluation Scope and Hypotheses.**
    *   **Goal:** To clearly state what we aim to measure with human evaluation, moving beyond simple correctness.
    *   **Hypotheses:**
        1.  **H1 (Coherence):** Models trained with `R_coherence` (`Pixelis-RFT-Coherence`, `Pixelis-RFT-Full`) will produce reasoning trajectories that are rated as significantly more logical and coherent by human judges than models trained without it (`Pixelis-RFT-Base`).
        2.  **H2 (Efficiency/Intelligence):** The `Pixelis-Online` model will be perceived by humans as more efficient and intelligent in its exploration strategy compared to the purely offline-trained `Pixelis-RFT-Full`.
*   **Task 2: Design the Human Evaluation Interface and Protocol.**
    *   **Goal:** To create a clear, simple, and unbiased interface for human annotators.
    *   **File:** `scripts/launch_human_eval_app.py` (a new script).
    *   **Action:**
        1.  **Build the Annotation UI:** Use `Gradio` or `Streamlit` to build the evaluation app. The interface will present an annotator with a visual input (image/video) and a question.
        2.  **Present Trajectories Blindly:** Crucially, it will display **two** reasoning trajectories side-by-side, generated by two different models for the same question (e.g., `RFT-Base` vs. `RFT-Full`). The annotator **will not know** which model produced which trajectory (this is a **blind review**).
        3.  **Define Annotation Criteria:** The annotator will be asked to evaluate both trajectories based on a 1-5 Likert scale for three criteria:
            *   **Correctness:** Is the final answer correct? (As a sanity check).
            *   **Coherence/Logicality:** Is the reasoning process logical, easy to follow, and free of contradictions or nonsensical steps?
            *   **Efficiency/Intelligence:** Does the agent seem intelligent? Does it explore relevant areas efficiently, or does it wander aimlessly?
*   **Task 2.5: Prepare the Data for Human Evaluation.**
    *   **Goal:** To sample and format the raw model outputs into blinded pairs for the evaluation app.
    *   **File:** `scripts/prepare_human_eval_data.py` (a new, simple script).
    *   **Action:** This script will take the full trajectory logs from `scripts/evaluate.py`, randomly sample N questions, create the blinded A/B pairs, and save them to a file that will serve as the direct input for the `launch_human_eval_app.py`.
*   **Task 3: Plan the Data Sampling and Annotation Process.**
    *   **Goal:** To ensure the evaluation is statistically meaningful and reliable.
    *   **Action:**
        1.  **Sample Size:** From your evaluation set (e.g., the Custom Capabilities Benchmark), randomly sample a diverse set of **at least 100-300 questions**. The final number will depend on your budget and resources, and this should be explicitly stated in `COMPUTE_BUDGENT.md` as "Human Annotation Budget".
        2.  **Multiple Annotators:** Each sampled question **must be annotated by three (3) different human judges** to ensure reliability and allow for the calculation of inter-annotator agreement.
        3.  **Annotator Training:** Prepare a clear guideline document for the annotators, with examples of good and bad trajectories for each criterion, to ensure they all share the same understanding of the task.
*   **Task 4: Analyze Results and Report Inter-Annotator Agreement.**
    *   **File:** `scripts/analyze_human_eval_results.py` (a new analysis script).
    *   **Action:**
        1.  **Collect and Aggregate Scores:** Collect all the ratings from the annotators.
        2.  Implement a Disagreement Resolution Protocol.
			* Goal: To improve the reliability of the final aggregated scores by handling cases with high annotator disagreement.
			* Protocol:
			* Identify High-Variance Samples: The analysis script must first identify all evaluation samples where the ratings from the three annotators show a high variance for key criteria (e.g., standard deviation > 1.5 on a 5-point scale). These are flagged as "disagreement cases".
			* Expert Adjudication: These flagged disagreement cases will be set aside from the main analysis pool and submitted to one or more designated "expert reviewers" (e.g., senior project members) for a final, binding adjudication.
			* Incorporate Adjudicated Scores: The final score from the expert reviewer will replace the original, high-variance scores for these specific cases before the final aggregation and statistical testing. This protocol must be clearly described in the methodology section of the final paper.
        3.  **Calculate Inter-Annotator Agreement (IAA):** The IAA (e.g., Fleiss' Kappa) will be calculated on the initial, raw ratings from the first three annotators, before the disagreement resolution step. This provides an unbiased measure of the clarity of the annotation guidelines. The adjudicated results are used for the final model comparison, not for inflating the IAA score.
        4.  **Perform Statistical Tests:** Compare the average scores for the different models (e.g., `RFT-Base` vs. `RFT-Full`). Use appropriate statistical tests (like a Wilcoxon signed-rank test, which is suitable for ordinal Likert scale data) to determine if the observed differences in scores are statistically significant.
        5.  **Report Findings:** The results will be presented in the paper with clear tables and discussion, including the IAA scores.
