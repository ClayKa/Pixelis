**Round 5: Final Analysis, Reporting, and Packaging**

*   **Task 1: Conduct In-Depth Analysis of Logged Metrics.**
    *   **Action:** Go back to the `wandb` logs from all experiments. Create plots that tell a story. For example, correlate a drop in the `R_coherence` reward signal with a specific type of task failure, or show how the `R_curiosity` reward spikes when the model is first introduced to a new domain.
*   **Task 2: Perform Qualitative Case Studies.**
    *   **Action:** Select a few compelling examples from your test set. Show the full reasoning trajectory generated by `Pixelis-RFT-Base` versus `Pixelis-Online`. Visualize where the `Pixelis-Online` model used its curiosity to explore a critical region that the base model missed. This qualitative evidence is extremely powerful.
*   **Task 3: Perform Systematic Error Mode Analysis.**
    *   **Action 1: Automated Discovery via Clustering.** Collect embeddings of all failure cases and use a clustering algorithm to automatically discover emergent error patterns.
    *   **Action 2: Manual Interpretation and Taxonomy Creation.** Manually inspect the representative samples from each cluster to interpret and name these data-driven patterns (e.g., "Fails on Low-Contrast Objects").
    *   **Action 3: Generate Comprehensive Report.** Combine the discovered patterns with your predefined manual taxonomy (Perception Failure, etc.) to create a final, comprehensive error analysis report, complete with visualizations and typical examples.
*   **Task 4: Create an Interactive Public Demonstrator.**
    *   **Goal:** To maximize the impact and accessibility of the research by allowing the community to experience the system's capabilities firsthand.
    *   **File:** `scripts/launch_demo.py`
    *   **Action:**
        *   **Build the UI:** Use `Gradio` or `Streamlit` to create a user-friendly web interface. The UI should allow users to upload an image/video and enter a question.
        *   **Implement Side-by-Side Comparison:** The core feature of the demo will be a side-by-side view that shows the real-time reasoning process of two or three key models from your ablation study (e.g., `Aura-RFT-Base` vs. `Aura-RFT-Full` vs. `Aura-Online`).
        *   **Visualize Trajectories:** As each model reasons, the interface should dynamically display the textual thoughts and visualize the visual operations (e.g., show the new cropped image after a `ZOOM-IN`).
        *   **Deploy:** Host the demo on a platform like Hugging Face Spaces for public access.
*   **Task 5: Create Comprehensive Documentation.**
    *   **Action:** Document the final architecture, the setup process, and crucially, provide a guide on how to **tune the reward weights (`w_curiosity`, `w_coherence`)** in the config file, as this is a key part of your framework.
        *   **ARCHITECTURE.md:** Write a detailed document explaining the high-level design choices. This should cover the rationale behind the asynchronous architecture, the plugin-based visual operation registry, the "Explore & Focus" reward system, and the online "Test-Time Evolution" engine.
        *   **BENCHMARKS.md:** Create a dedicated document to present all key experimental results. This will include the main ablation study tables, sample efficiency curves, hyperparameter sensitivity plots, and qualitative case study examples.
        *   **TROUBLESHOOTING.md:** Compile a list of common issues encountered during development and experimentation (e.g., CUDA OOM errors, slow training, common configuration mistakes) and provide clear, actionable solutions for each.
        *   **API_REFERENCE.md:** Use an automated documentation tool like Sphinx or `pdoc` to parse the docstrings from your Python code (`core/modules`) and generate a clean, professional API reference.
*   **Task 6: Package for Release with Mandated Artifact Management.**
    *   **Goal:** To package the project according to the highest standards of open-source scientific software, ensuring maximum reproducibility and linking all results to their sources.
    *   **Action 1: Package Source Code and Documentation.**
    *   **Action 2: Implement Mandatory Artifact Versioning.**
        *   **Platform:** Standardize on using **WandB Artifacts** or **Hugging Face Hub** for all experimental outputs.
        *   **Protocol:** The training and evaluation scripts (`scripts/train.py`, `scripts/evaluate.py`) **must be modified** to not only save outputs locally but also to log them as versioned artifacts. This includes:
            1.  **Datasets:** The exact version of the filtered training data used for a run.
            2.  **Configuration:** The Hydra config file for each run.
            3.  **Model Checkpoints:** Every saved LoRA adapter (`sft_adapters`, `rft_adapters_full`, etc.).
            4.  **Evaluation Results:** The final JSON/CSV files containing the evaluation metrics.
        *   **Rationale:** This creates an unbreakable, end-to-end chain of reproducibility. Anyone can go to a specific experiment run in WandB and find the exact data, code version, config, and model weights that produced the reported results.
    *   **Action 3: Create and Document a Minimal, End-to-End Reproducibility Kit.**
        *   **Goal:** To dramatically lower the barrier to entry for other researchers and developers, allowing them to quickly and easily verify your core results on consumer-grade hardware.
        *   **Action 3.1: Create a Minimal Dataset.**
            *   Create a "tiny" version of your synthesized dataset (e.g., 100 training samples, 50 validation samples) that captures the diversity of your tasks. This dataset will be packaged with the main repository.
        *   **Action 3.2: Provide Pre-trained Minimal Adapters.**
            *   Train your `Pixelis-SFT-Baseline` and `Pixelis-RFT-Full` models on this tiny dataset and save the resulting LoRA adapters. These small adapter files (~ a few MBs) will also be packaged with the repository.
        *   **Action 3.3: Create a "Run-from-Scratch" Guide.**
            *   **File:** Add a new, prominent section in the main `README.md` file.
            *   **Content:** This section will provide a simple, step-by-step list of commands that a user can copy and paste to:
                1.  Set up the environment.
                2.  Run a quick, end-to-end training pipeline on the tiny dataset, which should complete in a reasonable time on a single consumer GPU (e.g., RTX 4090).
                3.  Run the evaluation script on the pre-trained minimal adapters to immediately reproduce the key findings on the tiny test set.
        *   **Rationale:** This "reproducibility kit" is the gold standard for modern open-source research. It proves that your entire system works and allows the community to engage with your work in a hands-on manner.