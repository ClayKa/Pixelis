**Round 4: Focused Reward Calculation and Asynchronous Updates**

*   **Task 1: Integrate the Focused Reward Orchestrator.**
    *   In `core/engine/inference_engine.py`, import and initialize the **exact same** `RewardOrchestrator` class from `core/modules/reward_shaping.py` that was used in Phase 1. Its weights (`w_curiosity`, `w_coherence`) will be loaded from the config and will remain fixed.
*   **Task 2: Compute Online Rewards Based on Pseudo-Labels.**
    *   When the confidence gate allows an update, use the final ensembled answer (`Answer_final`) as a high-quality pseudo-label to calculate `R_final`.
    *   Use the model's initial reasoning trajectory to calculate `R_curiosity` and `R_coherence`.
*   **Task 3: Structure and Enqueue the Update Task.**
    *   Define an `UpdateTask` dataclass to neatly package all necessary information: the experience data, the computed multi-component reward tensor, and the determined learning rate (either `lr_correction` or `lr_adaptive`).
    *   Push this `UpdateTask` object onto the `update_queue` for the worker process to consume.
    *   When creating the `UpdateTask` dataclass object, ensure it includes the `original_logits` generated by the policy model during the initial inference pass. This is essential for the KL divergence calculation.
*   **Task 4: Implement a Conservative and Stable Model Update Worker.**
    *   **Goal:** To ensure the online learning process is robust against outlier data, prevents catastrophic forgetting, and maintains stable performance over time by employing a multi-layered safety mechanism.
    *   **File:** `core/engine/update_worker.py`, `docs/ARCHITECTURE.md`
    *   **Action 1: Implement the Core Update Loop.**
        *   The worker process will continuously fetch `UpdateTask` objects (which include `experience`, `reward_tensor`, `learning_rate`, and `original_logits`) from the `update_queue`.
        *   For each task, it will perform a forward and backward pass to compute the gradients.
    *   **Action 2: Define a Conservative, Self-Regulating Loss Function.**
        *   **Goal:** To dynamically control the trade-off between learning new information and preserving the base policy, preventing both policy drift and training stagnation.
        *   **File:** `core/engine/update_worker.py`, `core/config_schema.py`, `configs/training_params.yaml`, `docs/ARCHITECTURE.md`
        *   **Implementation:**
            1.  **Configuration:** In `core/config_schema.py`, add a `KLConfig` dataclass with fields like `beta_update_mode: Literal["fixed", "auto"]`, `initial_beta`, `target_kl`, `kl_tolerance`.
            2.  **State Tracking:** The `UpdateWorker` process will now maintain a running average of the KL divergence from recent updates (`mean_kl_divergence`).
            3.  **Loss Calculation:** The total loss is still `Total_Loss = RL_Loss + (current_beta * KL_Penalty)`.
            4.  **Automatic Beta Adjustment:** If the configured `beta_update_mode` is "auto", after each update step, an adjustment logic is applied:
                *   If `mean_kl_divergence > (target_kl + kl_tolerance)`, it means the policy is changing too fast. **Increase `current_beta`** (e.g., `current_beta *= 1.2`).
                *   If `mean_kl_divergence < (target_kl - kl_tolerance)`, it means the updates are too conservative and learning may have stalled. **Decrease `current_beta`** (e.g., `current_beta /= 1.2`).
        *   **Logging:** The value of `current_beta` must be logged to `wandb` to monitor its dynamic changes.
        *   **Documentation Mandate:** The exact mathematical formulation used for the KL Divergence penalty (`KL_Penalty`) **must** be documented in a dedicated section within `docs/ARCHITECTURE.md`. This documentation should clarify whether a forward or reverse KL formulation is used and reference the specific implementation (e.g., the TRL library's default or a custom function). This ensures scientific clarity and reproducibility.
    *   **Action 3: Apply Gradient Clipping for Exploding Gradient Prevention.**
        *   **Immediately after** the `Total_Loss.backward()` call and **before** the `optimizer.step()` call, apply gradient clipping.
        *   Use `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`. The `max_norm` value should also be a configurable parameter.
    *   **Action 4: Implement EMA Smoothing with a **Versioned and Atomic** Decoupled Synchronization Mechanism.**
        *   **Goal:** To smooth model updates while preventing resource contention and **eliminating the risk of race conditions during model synchronization.**
        *   **File:** `core/engine/update_worker.py`, `core/engine/inference_engine.py`, `configs/training_params.yaml`.
        *   **Implementation in `update_worker.py` (The Writer):**
            1.  After each `optimizer.step()`, the worker updates its local EMA model parameters.
            2.  Periodically..., the worker will save the complete `state_dict` of its stable EMA model using a **versioned, atomic write protocol**:
                a. Save the new model to a temporary file with a unique version identifier (e.g., a timestamp or a counter): `ema_model_snapshot.v2.pt.tmp`.
                b. Perform an **atomic `os.rename()`** to move the temporary file to its final versioned name: `os.rename("ema_model_snapshot.v2.pt.tmp", "ema_model_snapshot.v2.pt")`.
                c. **Atomically update a "pointer" file** or symbolic link that always points to the latest stable version. For example, write the new filename "ema_model_snapshot.v2.pt" into a file named `latest_model_version.txt`. This update must also be atomic (write to temp, then rename).
        *   **Implementation in `inference_engine.py` (The Reader):**
            1.  The inference engine also checks periodically...
            2.  When it's time to sync, it will **first read the pointer file** (`latest_model_version.txt`) to get the filename of the latest stable, complete snapshot.
            3.  It will then load the model weights **from that specific versioned file**, e.g., `torch.load("ema_model_snapshot.v2.pt")`.
        *   **Rationale:** This versioned, pointer-based protocol prevents the `inference_engine` from ever attempting to read a model snapshot file while the `update_worker` is in the middle of writing it. This completely eliminates the race condition and ensures that synchronization is always safe and loads a consistent model state.

    *   **Action 5: Log Update Contribution Metrics for Post-Hoc Analysis.**
        *   **Goal:** To record the impact of each individual online update, enabling deeper offline analysis of what types of experiences contribute most to performance changes.
        *   **File:** `core/engine/update_worker.py`.
        *   **Implementation:** In addition to the main `update_audit.log`, the worker will log a more detailed, structured log (e.g., `update_contribution.jsonl`) containing:
            1.  The `Experience` unique ID.
            2.  The full multi-component `reward_tensor`.
            3.  The calculated `KL_divergence` for this specific update.
            4.  **(Advanced)** To get `Î”validation`, a separate, lightweight evaluation process could be run periodically in the background on a small, held-out validation set. This process would evaluate both the "live" model and the "EMA" model and log their performance difference. This allows for direct measurement of how a series of recent updates has impacted overall performance.
    *   **Summary of the "Three-Tiered Safety System":**
        1.  **Behavioral Guardrail (KL Penalty):** Constrains *what* the model can learn by penalizing drastic policy shifts.
        2.  **Magnitude Guardrail (Gradient Clipping):** Constrains *how much* the model can learn from any single, potentially noisy, data point.
        3.  **Temporal Guardrail (EMA Smoothing):** Constrains *how fast* the learned updates are reflected in the user-facing model, ensuring smooth and stable performance evolution.